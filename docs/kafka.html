<h1>Apache Kafka</h1>

<ul>
<li><a href="#apache-kafka">Apache Kafka</a></li>
<li><a href="#1-writing-messages-to-kafka">1. Writing messages to Kafka</a>
<ul>
<li><a href="#11-producers">1.1. Producers</a></li>
<li><a href="#12-topics">1.2. Topics</a></li>
<li><a href="#13-partitions">1.3. Partitions</a></li>
<li><a href="#14-replication">1.4. Replication</a></li>
</ul></li>
<li><a href="#2-reading-data-from-kafka">2. Reading Data from Kafka</a>
<ul>
<li><a href="#21-consumers">2.1. Consumers</a></li>
<li><a href="#22-consumer-groups">2.2. Consumer groups</a></li>
<li><a href="#23-offsets">2.3. Offsets</a></li>
<li><a href="#24-heartbeat">2.4. Heartbeat</a></li>
</ul></li>
<li><a href="#3-kafka-internals">3. Kafka Internals</a>
<ul>
<li><a href="#31-broker">3.1. Broker</a></li>
<li><a href="#32-zookeeper">3.2. Zookeeper</a></li>
<li><a href="#33-log">3.3. Log</a></li>
<li><a href="#34-replication">3.4. Replication</a></li>
</ul></li>
<li><a href="#4-reliable-data-delivery">4. Reliable Data Delivery</a>
<ul>
<li><a href="#41-exactly-once-semantics">4.1. Exactly-once semantics</a></li>
<li><a href="#42-at-least-once-semantics">4.2. At-least-once semantics</a></li>
<li><a href="#43-at-most-once-semantics">4.3. At-most-once semantics</a></li>
</ul></li>
<li><a href="#5-building-data-pipelines">5. Building Data pipelines</a>
<ul>
<li><a href="#51-kafka-connect">5.1. Kafka Connect</a></li>
<li><a href="#52-kafka-streams">5.2. Kafka Streams</a></li>
</ul></li>
<li><a href="#6-monitoring-kafka">6. Monitoring Kafka</a>
<ul>
<li><a href="#61-jmx">6.1. JMX</a></li>
<li><a href="#62-prometheus">6.2. Prometheus</a></li>
<li><a href="#63-grafana">6.3. Grafana</a></li>
</ul></li>
<li><a href="#7-streaming-processing">7. Streaming Processing</a>
<ul>
<li><a href="#71-apache-spark">7.1. Apache Spark</a></li>
<li><a href="#72-apache-flink">7.2. Apache Flink</a></li>
<li><a href="#73-amazon-kinesis">7.3. Amazon Kinesis</a></li>
</ul></li>
</ul>

<h1>1. Writing messages to Kafka</h1>

<h2>1.1. Producers</h2>

<ul>
<li>Write messages to Kafka topics.</li>
<li>Can be configured to use a variety of serialization formats.</li>
</ul>

<h2>1.2. Topics</h2>

<ul>
<li>A logical grouping of messages.</li>
<li>Can be partitioned to improve scalability.</li>
</ul>

<h2>1.3. Partitions</h2>

<ul>
<li>A physical division of a topic.</li>
<li>Messages are distributed across partitions based on a hash of their key.</li>
</ul>

<h2>1.4. Replication</h2>

<ul>
<li>Each partition is replicated to a number of brokers.</li>
<li>This ensures that messages are not lost if a broker fails.</li>
</ul>

<h1>2. Reading Data from Kafka</h1>

<h2>2.1. Consumers</h2>

<ul>
<li>Read messages from Kafka topics.</li>
<li>Can be configured to use a variety of consumer groups.</li>
</ul>

<h2>2.2. Consumer groups</h2>

<ul>
<li>A group of consumers that read from the same topic.</li>
<li>Messages are distributed across consumers in the group.</li>
</ul>

<h2>2.3. Offsets</h2>

<ul>
<li>The position in a partition where a consumer is reading from.</li>
<li>Offsets are maintained by Kafka and are used to resume reading from where a consumer left off.</li>
</ul>

<h2>2.4. Heartbeat</h2>

<ul>
<li>A periodic message that consumers send to Kafka to indicate that they are still alive.</li>
<li>If a consumer fails to send a heartbeat, Kafka will mark it as dead and stop sending it messages.</li>
</ul>

<h1>3. Kafka Internals</h1>

<h2>3.1. Broker</h2>

<ul>
<li>A server that runs Kafka.</li>
<li>Brokers are responsible for storing messages, replicating them to other brokers, and serving them to consumers.</li>
</ul>

<h2>3.2. Zookeeper</h2>

<ul>
<li>A distributed coordination service that Kafka uses to store metadata about its topics, brokers, and consumers.</li>
</ul>

<h2>3.3. Log</h2>

<ul>
<li>A circular log that stores messages.</li>
<li>Messages are appended to the log in a append-only fashion.</li>
</ul>

<h2>3.4. Replication</h2>

<ul>
<li>Each partition is replicated to a number of brokers.</li>
<li>This ensures that messages are not lost if a broker fails.</li>
</ul>

<h1>4. Reliable Data Delivery</h1>

<h2>4.1. Exactly-once semantics</h2>

<ul>
<li>Guarantees that messages are delivered exactly once.</li>
<li>This is the most reliable delivery semantics, but it can be the most expensive.</li>
</ul>

<h2>4.2. At-least-once semantics</h2>

<ul>
<li>Guarantees that messages are delivered at least once.</li>
<li>This is a less expensive delivery semantics than exactly-once, but it can result in duplicate messages.</li>
</ul>

<h2>4.3. At-most-once semantics</h2>

<ul>
<li>Guarantees that messages are delivered at most once.</li>
<li>This is the least expensive delivery semantics, but it can result in lost messages.</li>
</ul>

<h1>5. Building Data pipelines</h1>

<h2>5.1. Kafka Connect</h2>

<ul>
<li>A framework for building and managing data pipelines that connect Kafka to other data sources and sinks.</li>
</ul>

<h2>5.2. Kafka Streams</h2>

<ul>
<li>A library for building real-time streaming applications that process data from Kafka.</li>
</ul>

<h1>6. Monitoring Kafka</h1>

<h2>6.1. JMX</h2>

<ul>
<li>A Java Management Extensions (JMX) console that can be used to monitor Kafka brokers.</li>
</ul>

<h2>6.2. Prometheus</h2>

<ul>
<li>An open-source monitoring system that can be used to collect metrics from Kafka brokers.</li>
</ul>

<h2>6.3. Grafana</h2>

<ul>
<li>A visualization tool that can be used to display metrics collected by Prometheus.</li>
</ul>

<h1>7. Streaming Processing</h1>

<h2>7.1. Apache Spark</h2>

<ul>
<li>A distributed processing framework that can be used to process streaming data from Kafka.</li>
</ul>

<h2>7.2. Apache Flink</h2>

<ul>
<li>A distributed processing framework that can be used to process streaming data from Kafka.</li>
</ul>

<h2>7.3. Amazon Kinesis</h2>

<ul>
<li>A fully-managed service that can be used to process streaming data from Kafka.</li>
</ul>
